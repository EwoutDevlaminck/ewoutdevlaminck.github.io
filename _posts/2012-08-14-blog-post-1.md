---
title: 'Sparse Identification of Nonlinear Dynamics'
date: 2023-02-10
permalink: /posts/2012/08/blog-post-1/
tags:
  - Machine learning
  - Sparsity
  - Model discovery
---

This blogpost attempts to give a comprehensive introduction to the work described mainly in the following three papers:

* ‘Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems’ [1]
* ‘Data-Driven Discovery of Coordinates and Governing Equations’ [2]
* ‘Data-Driven Discovery of Partial Differential Equations’. [3]

The work on SINDy and related projects is still very much ongoing and each year numerous breakthroughs are made. Here I just try to highlight the concept, as well as some of the initial successes that have been achieved.


Discovering governing equations – The need for SINDy
======
 > “Not only is the Universe stranger than we think, it is stranger than we can think.” ---<cite>Werner Heisenberg.</cite>


One of the main goals of science, is to describe the world in terms of theories and formulas. To discover models that capture the essence of vastly complex systems and give us the power to predict what will happen next, or how we can change it. In our pursuit for these models, the problems we tried to tackle have become increasingly harder. With the realisation that solving systems analytically is becoming more and more unfeasible, science has made a drastic migration to the computational domain ever since its power became available. 
For many of the hot topics in science today- climate science, neuroscience, ecology, finance epidemiology, etc.- data is abundant, but good models remain elusive. As the quote by Heisenberg suggests, maybe some of these systems surpass what we as humans are capable of grasping. Maybe, we need to rely on machines to help us discover models and dynamics. And what better tool to use, than machine learning?

Sparse Identification of Nonlinear Dynamics, or SINDy for short, tries to do exactly this. It builds on the observation that many natural systems governed by equations containing only a few terms (in an appropriate basis, that is) and tries to implement this. SINDy applies a combination of sparsity-promoting techniques and machine learning to nonlinear dynamical systems to discover governing equations from noisy measurement data. 
In the following, I will try to walk you through the working principles of SINDy on the basis of some of the problems they were able to solve.


Example- The Lorenz system
======
As a first example consider the Lorenz system ([Lorenz system wiki](https://en.wikipedia.org/wiki/Lorenz_system)). It was first introduced in the 1963 by Edward Lorenz, Ellen Fetter and Margaret Hamilton as a simplified model for atmospheric convection. Even though it is quite simple in structure, it was one of the first systems that popularized chaos theory, as the system becomes chaotic under some initial conditions.

![Lorenz](/images/Picture12.png)

SINDy is used to try and recover the system of coupled equations, based solely on a time history of the ‘state **x**(t)’ (here x,y,z). The input to the model consists of this state and its time derivative (either given or computed numerically). With this input, a library $\Theta$(**X**) is constructed, consisting of all the candidate nonlinear terms built from **X** we want to include. Each one of these terms can then show up in the resulting equations. With P<sub>n</sub> denoting polynomial terms of order n, a library may look like this:

![Library](/images/Picture13.png)
 

The sparse regression problem that is subsequantly solved using standard machine learning techniques, is simply stated as 

![Lorenz](/images/Picture14.png)
 
We try to find the sparse vectors of coefficients given by $\Xi$. This is a sparse matrix, with the nonzero elements indicating what terms from the candidate library should be present in the system of equations. Essentially, for every coordinate of the original problem a sparse regression has to be solved in this manner. This already highlights one of the major limitations of this model, as a solution quickly becomes unfeasible as the number of coordinates grows.

![Lorenz_sindy](/images/Picture1.png)

---<cite>‘Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems’ [1]</cite>

The model for the Lorenz system is shown schematically above. The function library contains mixed polynomial terms up to order 5 and the $\Xi$ matrix is clearly very sparse, with only a couple of nonzero coefficients. Even though the input data is noisy (mainly caused by the numerical errors introduced in taking the time derivative), the true model is retrieved exceptionally well, with all the correct terms being identified, as well as the magnitudes of the coefficients.



Example - Von Karman vortex street
=====
 As a second example of the method, the original paper discusses the modelling of fluid flow around a cylinder. For low Reynold's number (Re~100), the flow organizes into a well known Von Karman vortex street quickly. In principle, the flow field is defined by it's value on the whole two-dimensional grid of the simulated test data. As mentioned in the previous section though, this would mean that we have to solve a regression optimization for each point, a task that is infeasible and highly overcomplicated.
 In order to tackle the problem more efficently -and this is an important remark in general for the SINDy method and alternatives-, we can include prior knowlegde of the system. We have, after all, almost always some available information on the system of interest. However strong our computers or models may become, it is the task of a scientist to guide the process.

 A very adequate method that can be used here, is the proper orthogonal decomposition [POD](https://en.wikipedia.org/wiki/Proper_orthogonal_decomposition) (otherwise known as PCA). POD provides a low-rank orthonormal basis for the problem, where the lowest order modes contain the most information and energy of the system, by explaining the highest amount of variance in the data. Essentially, the lowest order modes give insight into what part of the system changes most, and therefore holds the most information. There is an extensive history of theoretical work around this and it can be shown that when we approximate the vortex shedding by including only the two most energetic modes, as well as a so-called 'shift mode', the mean field evolves according to the following equations:

![Kamrna_system](/images/Picture4.png)


Here, x,y and z are the first, second and shift mode respectively. The evolution of this system of equations is shown in the figure below, as well as a representation of the different modes. The indicated limit cycle describes the case where the Von Karamn vortex street is fully developed and the mean field alternates between the first and second mode, with a strong presence of the shift mode.

![Karman_sindy](/images/Picture3.png)

A time series of these three coordinates was given to SINDy and again the original dynamics were recovered very accurately. Interestingly, the theoretical solution to this problem took over 15 years to reach consensus. Here, with the help of SINDy and some background knowledge on the system, the solution was recovered right away.



Specifics of SINDy
=====

Optimization and sparseness
------

So far, my focus has been on intuitively introducing the method, without going into too much technical detail. By now, you hopefully have some intuition on how the technique approaches model discovery. Before going further and talking about extensions and updates to SINDy, there are two burning questions that perhaps require answering: How does it actually optimize, and how does one include the requirement of sparseness?

Both of these questions are closely related and the key lies in the specific form of the loss function. Everyone who is familiar with machine learning should have apretty good idea of what a loss function is and how it is a central part of the optimization. SINDy uses a pointwise l<sub>2</sub> loss between the ground truth and prediction (as is very standard for regression problems), but extends this with a sparsity promoting term. The loss functions looks something like this:

![loss1](/images/Picture15.png)

The first term makes sure the regression is accurate, while the second term penalizes the presence of nonzero terms in the coefficient matrix. The form shown here corresponds to the least absolute shrinkage and selection operator (LASSO), basically providing l<sub>1</sub> regularization. In practice however, this is not what is applied in SINDy as it can become really expensive to calculate for large datasets. Instead, a so-called sequential thresholded least-squares method is used. In this method, terms of the coefficient matrix that are below a certain threshold are set to zero and removed/made inactive, after which the least-squares regression is performed again. This is done several times iteratively until sufficient sparsity is achieved and the resulting solution is very similar to what the loss function above would lead to.

The $\lambda$ parameter, as well as the actual threshold value for deletion give the needed flexibility for the SINDy algorithm and how strong we want to regularize the problem.

The function library
------



Finding coordinates
=====



Autoencoders
------

![auto](/images/Picture7.png)

![full_syndy](/images/Picture5.png)



Examples
------


![first](/images/Picture8.png)
![second](/images/Picture9.png)



Optimization
------

![loss](/images/Picture6.png)


Further improvements - PDE-FIND
=====

![first](/images/Picture11.png)


Conclusions
=====


Bibliography
=====

* [1] Brunton, Steven L., Joshua L. Proctor, and J. Nathan Kutz. ‘Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems’. Proceedings of the National Academy of Sciences 113, no. 15 (12 April 2016): 3932–37. https://doi.org/10.1073/pnas.1517384113.

* [2] Champion, Kathleen, Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. ‘Data-Driven Discovery of Coordinates and Governing Equations’. Proceedings of the National Academy of Sciences 116, no. 45 (5 November 2019): 22445–51. https://doi.org/10.1073/pnas.1906995116.

* [3] Rudy, Samuel H., Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. ‘Data-Driven Discovery of Partial Differential Equations’. Science Advances 3, no. 4 (7 April 2017): e1602614. https://doi.org/10.1126/sciadv.1602614.

